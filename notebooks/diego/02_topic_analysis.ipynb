{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "DATA_DIR = Path('/Users/darenasc/projects/un-speeches/data')\n",
    "df_speech_url = pd.read_csv(DATA_DIR / \"UN Speeches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc: str) -> str:\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "def get_corpus_from_file(country: str, path: Path = DATA_DIR / \"2023\") -> str:\n",
    "    with open(path / f\"{country}.json\") as f:\n",
    "        json_data = json.load(f)\n",
    "    corpus = [x['text'] for x in json_data]\n",
    "    large_corpus = ' '.join([x for x in corpus])\n",
    "    return large_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_corpus_from_file(df_speech_url.iloc[0]['country'])\n",
    "corpus = clean(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = []\n",
    "for i, r in df_speech_url.iterrows():\n",
    "    try:\n",
    "        corpus = get_corpus_from_file(r['country'])\n",
    "        corpus = clean(corpus)\n",
    "        all_text.append(corpus)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean = [clean(doc).split() for doc in all_text] \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"country\" + 0.009*\"international\" + 0.007*\"world\" + 0.007*\"peace\" + 0.007*\"people\" + 0.006*\"security\" + 0.006*\"nation\" + 0.006*\"president\" + 0.006*\"development\" + 0.006*\"assembly\"'),\n",
       " (1,\n",
       "  '0.017*\"russia\" + 0.012*\"ukraine\" + 0.008*\"war\" + 0.006*\"must\" + 0.006*\"ukrainian\" + 0.005*\"aggression\" + 0.005*\"crime\" + 0.004*\"nation\" + 0.004*\"world\" + 0.004*\"food\"'),\n",
       " (2,\n",
       "  '0.012*\"country\" + 0.009*\"world\" + 0.008*\"people\" + 0.007*\"nation\" + 0.006*\"u\" + 0.006*\"state\" + 0.006*\"united\" + 0.005*\"one\" + 0.004*\"international\" + 0.004*\"development\"'),\n",
       " (3,\n",
       "  '0.009*\"nation\" + 0.009*\"country\" + 0.008*\"president\" + 0.007*\"global\" + 0.006*\"world\" + 0.006*\"international\" + 0.006*\"united\" + 0.006*\"assembly\" + 0.006*\"development\" + 0.006*\"peace\"'),\n",
       " (4,\n",
       "  '0.008*\"ecuador\" + 0.004*\"malnutrition\" + 0.002*\"child\" + 0.002*\"organized\" + 0.002*\"infant\" + 0.002*\"chronic\" + 0.002*\"galapagus\" + 0.002*\"prioritized\" + 0.001*\"transnational\" + 0.001*\"figure\"')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=5, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darenasc/.local/share/virtualenvs/un-speeches-OCicuzVT/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1710: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "central asia global afghanistan development security region participant cooperation issue\n",
      "Topic 1:\n",
      "russia ukraine aggression war crime ukrainian security global nuclear council\n",
      "Topic 2:\n",
      "thing decision different time think make today right lot need\n",
      "Topic 3:\n",
      "africa african continent global united security council economic democratic war\n",
      "Topic 4:\n",
      "uh um strengthen like african union continent security area order\n",
      "Topic 5:\n",
      "island ocean taiwan climate global challenge change sea pacific mr\n",
      "Topic 6:\n",
      "sudan south education child republic arm health embargo ensure implementation\n",
      "Topic 7:\n",
      "trafficking drug child crime weve organized human organization phenomenon united\n",
      "Topic 8:\n",
      "development global mr united developing agenda sdgs sustainable support progress\n",
      "Topic 9:\n",
      "refugee palestinian syrian crisis majesty help security arab host solution\n",
      "Topic 10:\n",
      "haitian haiti net climate zero mr carbon energy republic kenya\n",
      "Topic 11:\n",
      "inequality need conflict multilateral civilian congo crucial crisis democratic particular\n",
      "Topic 12:\n",
      "climate need migration europe green industrial right africa work partnership\n",
      "Topic 13:\n",
      "japan nuclear dignity weapon human disarmament security strengthen zero rule\n",
      "Topic 14:\n",
      "korea digital divide republic freedom energy security technology global child\n",
      "Topic 15:\n",
      "city libya disaster speaking condolence tragedy god set eye unity\n",
      "Topic 16:\n",
      "democracy eu european ukraine stand europe neighbor union membership war\n",
      "Topic 17:\n",
      "society gentleman lady migrant development ethnic equal democracy government trust\n",
      "Topic 18:\n",
      "iran region order certain seen yemen security west terrorist war\n",
      "Topic 19:\n",
      "cuba south united right development economic measure today continue financial\n",
      "\n",
      "Topic 0:\n",
      "global security development united climate effort republic challenge support change\n",
      "Topic 1:\n",
      "global united climate development security need democratic government action african\n",
      "Topic 2:\n",
      "development global right mr government united resource today change effort\n",
      "Topic 3:\n",
      "development global united security need climate progress republic support action\n",
      "Topic 4:\n",
      "united development human global security law conflict need time sustainable\n",
      "Topic 5:\n",
      "development uh security africa african global today united like new\n",
      "Topic 6:\n",
      "development security african challenge action united need climate right refugee\n",
      "Topic 7:\n",
      "el security solidarity right wish time government global decision uh\n",
      "Topic 8:\n",
      "global security united support challenge time future mr right war\n",
      "Topic 9:\n",
      "security global development need region member united climate uh support\n",
      "Topic 10:\n",
      "time economic africa security japan weapon community support nuclear global\n",
      "Topic 11:\n",
      "united need mr security climate global development time goal change\n",
      "Topic 12:\n",
      "development united security global climate mr republic support need change\n",
      "Topic 13:\n",
      "climate change right mr humankind policy germany agenda basis plan\n",
      "Topic 14:\n",
      "united development uh need support order economic right cooperation security\n",
      "Topic 15:\n",
      "united global security need right war development climate human time\n",
      "Topic 16:\n",
      "development republic global africa economic support change challenge security effort\n",
      "Topic 17:\n",
      "united global development mr republic government human sustainable climate security\n",
      "Topic 18:\n",
      "development right democracy political future challenge society human global need\n",
      "Topic 19:\n",
      "war russia global ukraine united threat nuclear food effect security\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = [doc for doc in all_text]\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=5, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print()\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "un-speeches-OCicuzVT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
