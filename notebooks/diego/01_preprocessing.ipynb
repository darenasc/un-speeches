{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "DATA_DIR = Path().absolute().parent.parent / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Download the data from google spredsheets\n",
    "df_speech_url = pd.read_csv(DATA_DIR / \"UN Speeches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc: str) -> str:\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "def get_corpus_from_file(country: str, path: Path = DATA_DIR / \"2023\") -> str:\n",
    "    with open(path / f\"{country}.json\") as f:\n",
    "        json_data = json.load(f)\n",
    "    corpus = [x['text'] for x in json_data]\n",
    "    large_corpus = ' '.join([x for x in corpus])\n",
    "    return large_corpus\n",
    "\n",
    "def get_transcript(video_id: str, start: str = None, end: str = None) -> list[str]:\n",
    "    # TODO Add filter with start time and end time, to cut introductions.\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        corpus = [x['text'] for x in transcript]\n",
    "        large_corpus = ' '.join([x for x in corpus])\n",
    "        return transcript, large_corpus\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, None\n",
    "\n",
    "def save_json(data: dict, country: str, output_path: Path = DATA_DIR / \"2023\"):\n",
    "    with open(output_path / f\"{country}.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "def download_speech_transcriptions(df_speech_url: pd.DataFrame, overwrite: bool = False):\n",
    "    pbar = tqdm(df_speech_url.iterrows(), total=len(df_speech_url))\n",
    "    for i, r in pbar:\n",
    "        pbar.set_description(r['country'])\n",
    "        if (DATA_DIR / \"2023\" / f\"{r['country']}.json\").is_file() and not overwrite:\n",
    "            continue\n",
    "        transcript, large_corpus = get_transcript(r['url'].split('?v=')[-1])\n",
    "        if transcript:\n",
    "            save_json(transcript, r['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_speech_transcriptions(df_speech_url, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate masks for the countries\n",
    "\n",
    "# countries shape https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip\n",
    "geo_data = gpd.read_file('/Users/darenasc/projects/un-speeches/data/ne_110m_admin_0_countries.zip')\n",
    "\n",
    "# Generate masks\n",
    "pbar = tqdm(df_speech_url.iterrows(), total=len(df_speech_url))\n",
    "for i, r in pbar:\n",
    "    pbar.set_description(r['country'])\n",
    "    if len(geo_data[geo_data['SOVEREIGNT']==r['country']]) > 0:\n",
    "        ax = geo_data[geo_data['SOVEREIGNT']==r['country']].plot();\n",
    "        ax.axis('off');\n",
    "        ax.figure.savefig(DATA_DIR / \"masks\" / f\"{r['country']}.jpg\");\n",
    "    else:\n",
    "        print(f\"{r['country']} not found.\")\n",
    "\n",
    "\"\"\"\n",
    "Palau not found.\n",
    "Monaco not found.\n",
    "Marshall Islands not found.\n",
    "Sao Tome & Principe not found.\n",
    "Comoros not found.\n",
    "Dominica not found.\n",
    "Kiribati not found.\n",
    "Micronesia not found.\n",
    "Palestine not found.\n",
    "Nauru not found.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_data[geo_data['SOV_A3'].isin(['COM'])]\n",
    "# geo_data['SOVEREIGNT'].unique()\n",
    "# geo_data[geo_data['SOVEREIGNT']=='United States of America']\n",
    "geo_data[geo_data['SOVEREIGNT']=='Libya'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud generation\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "pbar = tqdm(df_speech_url.iterrows(), total=len(df_speech_url))\n",
    "for i, r in pbar:\n",
    "    pbar.set_description(r['country'])\n",
    "    try:\n",
    "        country_mask = np.array(Image.open(DATA_DIR / \"masks\" / f\"{r['country']}.jpg\"))\n",
    "        wc = WordCloud(background_color=\"white\", max_words=2000, mask=country_mask, stopwords=stopwords, contour_width=3, contour_color='steelblue')\n",
    "        corpus = get_corpus_from_file(r['country'])\n",
    "        corpus = clean(corpus)\n",
    "        wc.generate(corpus)\n",
    "        wc.to_file(DATA_DIR / \"wordclouds\" / f\"{r['country']}_words.png\");\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to download the stopwords\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "# nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.draw.dispersion import dispersion_plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispersion_plot(text, words, ignore_case=False, title=\"Lexical Dispersion Plot\"):\n",
    "    \"\"\"\n",
    "    Generate a lexical dispersion plot.\n",
    "\n",
    "    :param text: The source text\n",
    "    :type text: list(str) or iter(str)\n",
    "    :param words: The target words\n",
    "    :type words: list of str\n",
    "    :param ignore_case: flag to set if case should be ignored when searching text\n",
    "    :type ignore_case: bool\n",
    "    :return: a matplotlib Axes object that may still be modified before plotting\n",
    "    :rtype: Axes\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"The plot function requires matplotlib to be installed. \"\n",
    "            \"See https://matplotlib.org/\"\n",
    "        ) from e\n",
    "\n",
    "    word2y = {\n",
    "        word.casefold() if ignore_case else word: y\n",
    "        # for y, word in enumerate((words))\n",
    "        for y, word in enumerate(reversed(words))\n",
    "    }\n",
    "    xs, ys = [], []\n",
    "    for x, token in enumerate(text):\n",
    "        token = token.casefold() if ignore_case else token\n",
    "        y = word2y.get(token)\n",
    "        if y is not None:\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "    words = words[::-1]\n",
    "    _, ax = plt.subplots()\n",
    "    ax.plot(xs, ys, \"|\")\n",
    "    ax.set_yticks(list(range(len(words))), words, color=\"C0\")\n",
    "    ax.set_ylim(-1, len(words))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Word Offset\")\n",
    "    return ax\n",
    "\n",
    "dispersion_plot(text, [str(w) for w, f in fdist.most_common(10)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "un-speeches-OCicuzVT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
