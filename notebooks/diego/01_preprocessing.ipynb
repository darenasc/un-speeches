{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "DATA_DIR = Path().absolute().parent.parent / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_un_spreadsheet = 'https://docs.google.com/spreadsheets/d/1qtqfnRSW24j-XLN7SRKywDCuFatARCH8pUg1Rr6I2vI/export?format=csv'\n",
    "response = requests.get(url_un_spreadsheet)\n",
    "with open(DATA_DIR / 'UN Speeches.csv', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "df_speech_url = pd.read_csv(DATA_DIR / \"UN Speeches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc: str) -> str:\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "def get_corpus_from_file(country: str, start: int = 0, end:int = 3600, path: Path = DATA_DIR / \"2023\") -> str:\n",
    "    with open(path / f\"{country}.json\") as f:\n",
    "        json_data = json.load(f)\n",
    "    corpus = [x['text'] for x in json_data if x['start'] > start and x['start'] < end]\n",
    "    large_corpus = ' '.join([x for x in corpus])\n",
    "    return large_corpus\n",
    "\n",
    "def get_transcript(video_id: str, start: str = None, end: str = None) -> list[str]:\n",
    "    # TODO Add filter with start time and end time, to cut introductions.\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        corpus = [x['text'] for x in transcript]\n",
    "        large_corpus = ' '.join([x for x in corpus])\n",
    "        return transcript, large_corpus\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, None\n",
    "\n",
    "def get_video_url(country: str) -> str:\n",
    "    if isinstance(df_speech_url[df_speech_url[\"country\"] == country][\"start\"].values[0], str):\n",
    "        h,m,s=df_speech_url[df_speech_url[\"country\"] == country][\"start\"].values[0].split(\":\")\n",
    "        seconds = int(h)*3600+int(m)*60+int(s)\n",
    "        url = f'{df_speech_url[df_speech_url[\"country\"] == country][\"url\"].values[0]}&t={seconds}'\n",
    "        return url\n",
    "    else:\n",
    "        url = df_speech_url[df_speech_url[\"country\"] == country][\"url\"].values[0]\n",
    "        return url\n",
    "    \n",
    "def save_json(data: dict, country: str, output_path: Path = DATA_DIR / \"2023\"):\n",
    "    with open(output_path / f\"{country}.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "def download_speech_transcriptions(df_speech_url: pd.DataFrame, overwrite: bool = False):\n",
    "    pbar = tqdm(df_speech_url.iterrows(), total=len(df_speech_url))\n",
    "    for i, r in pbar:\n",
    "        pbar.set_description(r['country'])\n",
    "        if (DATA_DIR / \"2023\" / f\"{r['country']}.json\").is_file() and not overwrite:\n",
    "            continue\n",
    "        transcript, large_corpus = get_transcript(r['url'].split('?v=')[-1])\n",
    "        if transcript:\n",
    "            save_json(transcript, r['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_speech_transcriptions(df_speech_url, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate masks for the countries\n",
    "\n",
    "# countries shape https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip\n",
    "geo_data = gpd.read_file(DATA_DIR / 'ne_110m_admin_0_countries.zip')\n",
    "\n",
    "# Generate masks\n",
    "pbar = tqdm(df_speech_url.iterrows(), total=len(df_speech_url))\n",
    "for i, r in pbar:\n",
    "    pbar.set_description(r['country'])\n",
    "    if len(geo_data[geo_data['SOVEREIGNT']==r['country']]) > 0:\n",
    "        ax = geo_data[geo_data['SOVEREIGNT']==r['country']].plot();\n",
    "        ax.axis('off');\n",
    "        ax.figure.savefig(DATA_DIR / \"masks\" / f\"{r['country']}.jpg\");\n",
    "    else:\n",
    "        print(f\"{r['country']} not found.\")\n",
    "\n",
    "\"\"\"\n",
    "Palau not found.\n",
    "Monaco not found.\n",
    "Marshall Islands not found.\n",
    "Sao Tome & Principe not found.\n",
    "Comoros not found.\n",
    "Dominica not found.\n",
    "Kiribati not found.\n",
    "Micronesia not found.\n",
    "Palestine not found.\n",
    "Nauru not found.\n",
    "Liechtenstein not found.\n",
    "Mauritius not found.\n",
    "Malta not found.\n",
    "Barbados not found.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_data[geo_data['SOV_A3'].isin(['COM'])]\n",
    "# geo_data['SOVEREIGNT'].unique()\n",
    "# geo_data[geo_data['SOVEREIGNT']=='United States of America']\n",
    "geo_data[geo_data['SOVEREIGNT']=='Libya'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud generation\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "pbar = tqdm(df_speech_url.iterrows(), total=len(df_speech_url))\n",
    "for i, r in pbar:\n",
    "    pbar.set_description(r['country'])\n",
    "    try:\n",
    "        country_mask = np.array(Image.open(DATA_DIR / \"masks\" / f\"{r['country']}.jpg\"))\n",
    "        wc = WordCloud(background_color=\"white\", max_words=2000, mask=country_mask, stopwords=stopwords, contour_width=3, contour_color='steelblue')\n",
    "        \n",
    "        if isinstance(r['start'], str) and isinstance(r['end'], str):\n",
    "            h_start, m_start, s_start = r['start'].values[0].split(':')\n",
    "            start = int(h_start) * 60*60 + int(m_start)*60 + int(s_start)\n",
    "            h_end, m_end, s_end = r['end'].values[0].split(':')\n",
    "            end = int(h_end) * 60*60 + int(m_end)*60 + int(s_end)\n",
    "            corpus = get_corpus_from_file(r['country'], start=start, end=end)\n",
    "        elif isinstance(r['start'], str) :\n",
    "            h, m, s = r['start'].values[0].split(':')\n",
    "            start = int(h) * 60*60 + int(m)*60 + int(s)\n",
    "            corpus = get_corpus_from_file(r['country'], start=start)\n",
    "        else:\n",
    "            corpus = get_corpus_from_file(r['country'])\n",
    "        corpus = clean(corpus)\n",
    "        wc.generate(corpus)\n",
    "        wc.to_file(DATA_DIR / \"wordclouds\" / f\"{r['country']}_words.png\");\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to download the stopwords\n",
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "# # nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# # nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.draw.dispersion import dispersion_plot\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispersion_plot(text, words, ignore_case=False, title=\"Lexical Dispersion Plot\"):\n",
    "    \"\"\"\n",
    "    Generate a lexical dispersion plot.\n",
    "\n",
    "    :param text: The source text\n",
    "    :type text: list(str) or iter(str)\n",
    "    :param words: The target words\n",
    "    :type words: list of str\n",
    "    :param ignore_case: flag to set if case should be ignored when searching text\n",
    "    :type ignore_case: bool\n",
    "    :return: a matplotlib Axes object that may still be modified before plotting\n",
    "    :rtype: Axes\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"The plot function requires matplotlib to be installed. \"\n",
    "            \"See https://matplotlib.org/\"\n",
    "        ) from e\n",
    "\n",
    "    word2y = {\n",
    "        word.casefold() if ignore_case else word: y\n",
    "        # for y, word in enumerate((words))\n",
    "        for y, word in enumerate(reversed(words))\n",
    "    }\n",
    "    xs, ys = [], []\n",
    "    for x, token in enumerate(text):\n",
    "        token = token.casefold() if ignore_case else token\n",
    "        y = word2y.get(token)\n",
    "        if y is not None:\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "    words = words[::-1]\n",
    "    _, ax = plt.subplots()\n",
    "    ax.plot(xs, ys, \"|\")\n",
    "    ax.set_yticks(list(range(len(words))), words, color=\"C0\")\n",
    "    ax.set_ylim(-1, len(words))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Word Offset\")\n",
    "    return ax\n",
    "\n",
    "# dispersion_plot(text, [str(w) for w, f in fdist.most_common(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "un-speeches-OCicuzVT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
